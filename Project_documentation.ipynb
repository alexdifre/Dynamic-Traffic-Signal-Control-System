{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe762e2f",
   "metadata": {},
   "source": [
    "# Dynamic Traffic Signal Control System- Project Documentation\n",
    "\n",
    "## Specification Document\n",
    "\n",
    "### Programming Language\n",
    "- **Main language:** Python\n",
    "- **Peer review languages:** Python\n",
    "\n",
    "### Problem Statement\n",
    "This project implements a reinforcement learning solution for optimizing traffic light control at a two-way intersection. The goal is to minimize vehicle waiting times and reduce traffic congestion by training an intelligent agent to make optimal signal timing decisions.\n",
    "\n",
    "### Algorithms and Data Structures\n",
    "\n",
    "**Core Algorithm:**\n",
    "- **Q-Learning**: A model-free reinforcement learning algorithm that learns the optimal action-value function through temporal difference learning.\n",
    "\n",
    "**Data Structures:**\n",
    "- **Dictionary (Hash Map)**: Used to store Q-values for state-action pairs\n",
    "  - Keys: `(state, action)` tuples\n",
    "  - Values: floating-point Q-values\n",
    "\n",
    "**Supporting Components:**\n",
    "- Traffic simulation environment with discrete state space\n",
    "- Epsilon-greedy action selection for exploration-exploitation balance\n",
    "\n",
    "### Input and Usage\n",
    "The program receives:\n",
    "- **Episode count**: Number of training/evaluation episodes to run\n",
    "- **Visualization flag**: Boolean to enable/disable GUI\n",
    "- **Pre-trained model**: Saved Q-table loaded from file for evaluation\n",
    "\n",
    "The state space consists of:\n",
    "- Traffic signal phase (binary: 0 or 1)\n",
    "- Vehicle count in lane A\n",
    "- Vehicle count in lane B  \n",
    "- Intersection occupancy (boolean)\n",
    "\n",
    "### Time and Space Complexity\n",
    "\n",
    "**Q-Learning Algorithm:**\n",
    "- **Time complexity per step:** O(|A|) where |A| is the action space size (2 in this case)\n",
    "  - Action selection: O(|A|) to find maximum Q-value\n",
    "  - Q-value update: O(1) dictionary operation\n",
    "- **Space complexity:** O(|S| × |A|) where |S| is the number of unique states encountered\n",
    "  - Stores one Q-value per state-action pair\n",
    "\n",
    "**Training:**\n",
    "- Total training time: O(E × T × |A|) where E = episodes, T = steps per episode\n",
    "\n",
    "### Sources\n",
    "- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.)\n",
    "- Watkins, C. J., & Dayan, P. (1992). Q-learning. *Machine Learning*, 8(3-4), 279-292.\n",
    "\n",
    "Master's Degree in Data science.\n",
    "---\n",
    "\n",
    "## Implementation Document\n",
    "\n",
    "### Program Structure\n",
    "\n",
    "The implementation consists of four main modules:\n",
    "\n",
    "1. **`Q_learn.py`**: Core Q-learning agent implementation\n",
    "   - Maintains Q-table as dictionary\n",
    "   - Implements epsilon-greedy policy\n",
    "   - Performs temporal difference learning updates\n",
    "\n",
    "2. **`environment.py`**: RL environment wrapper\n",
    "   - Interfaces with traffic simulator\n",
    "   - Constructs state observations from simulation\n",
    "   - Calculates rewards based on queue reduction\n",
    "   - Manages episode lifecycle\n",
    "\n",
    "3. **`utils.py`**: Training and evaluation utilities\n",
    "   - Orchestrates training loops\n",
    "   - Handles model persistence (save/load Q-tables)\n",
    "   - Runs evaluation metrics collection\n",
    "   - Reports performance statistics\n",
    "\n",
    "4. **`TrafficSimulator`**: External traffic simulation engine (not shown)\n",
    "   - Provides realistic traffic flow dynamics\n",
    "   - Renders visualization when requested\n",
    "\n",
    "### Achieved Complexities\n",
    "\n",
    "- **Time per learning step:** O(|A|) = O(2) = O(1) in practice\n",
    "- **Space:** O(|S| × |A|) where |S| grows with encountered states\n",
    "\n",
    "**Training Loop:**\n",
    "- For 10,000 episodes with average 100 steps per episode:\n",
    "- Total operations: O(10,000 × 100 × |A|) = O(2,000,000) = practical O(1) per step\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "The reward function incentivizes queue reduction:\n",
    "```python\n",
    "reward = previous_queue_size - current_queue_size\n",
    "```\n",
    "\n",
    "This provides:\n",
    "- **Positive reward** when traffic clears (effective control)\n",
    "- **Negative reward** when queues grow (poor control)\n",
    "- **Zero reward** for stable state\n",
    "\n",
    "The epsilon-greedy policy (ε = 0.1) balances:\n",
    "- 90% exploitation of learned policy\n",
    "- 10% random exploration for discovering improvements\n",
    "\n",
    "\n",
    "### Use of Large Language Models\n",
    "\n",
    "**large language models were used only partially in the development of this code.** \n",
    "\n",
    "All implementation decisions, algorithm design, and code structure were created through traditional software engineering practices and consultation of academic reinforcement learning literature.\n",
    "\n",
    "They were used to rewrite the code components and comments in an orderly and clear manner, each section was then reviewed by hand to ensure the correctness of what was generated\n",
    "\n",
    "### Sources\n",
    "- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.\n",
    "- Watkins, C. J. (1989). *Learning from delayed rewards* (Doctoral dissertation). University of Cambridge.\n",
    "- Traffic simulation concepts based on standard discrete-event simulation principles\n",
    "\n",
    "---\n",
    "\n",
    "## Testing Document\n",
    "\n",
    "### Unit Testing Coverage\n",
    "\n",
    "The test suite (`test_environment.py`) provides comprehensive testing of critical RL environment functionality with 6 targeted test cases covering:\n",
    "- Reward calculation correctness\n",
    "- Simulation execution and state caching\n",
    "- State structure validation\n",
    "- Multi-step reward consistency\n",
    "- Episode termination conditions\n",
    "- Environment reset behavior\n",
    "\n",
    "### Testing Methodology\n",
    "\n",
    "**Test Framework:** Python `unittest` with `unittest.mock` for dependency isolation\n",
    "\n",
    "**Key Test Cases:**\n",
    "\n",
    "1. **`test_1_reward_calculation_correctness`**\n",
    "   - **Purpose:** Validates reward function mathematical correctness\n",
    "   - **Method:** Tests three scenarios with known queue changes\n",
    "   - **Inputs:** \n",
    "     - Improvement: 10 → 5 vehicles (expected reward: +5.0)\n",
    "     - Worsening: 5 → 12 vehicles (expected reward: -7.0)\n",
    "     - No change: 8 → 8 vehicles (expected reward: 0.0)\n",
    "   - **Assertion:** `assertEqual` on exact reward values\n",
    "\n",
    "2. **`test_2_step_executes_simulation_and_updates_cache`**\n",
    "   - **Purpose:** Ensures simulation advances and state cache updates\n",
    "   - **Method:** Mocks simulator and verifies method calls\n",
    "   - **Inputs:** Action signal, mocked environment with 8 vehicles\n",
    "   - **Assertions:**\n",
    "     - `mock_sim.run.assert_called_once_with(1)` - simulation ran\n",
    "     - Cache updated to current vehicle count\n",
    "     - Return types correct (tuple, float, bool, bool)\n",
    "\n",
    "3. **`test_3_state_structure_correctness`**\n",
    "   - **Purpose:** Validates state tuple structure for agent compatibility\n",
    "   - **Method:** Constructs state with known configuration\n",
    "   - **Inputs:** Mocked lanes with 4 and 6 vehicles, signal phase 1\n",
    "   - **Expected:** `(1, 4, 6, bool)` - exactly 4 elements in correct order\n",
    "   - **Assertions:** Length check and element type validation\n",
    "\n",
    "4. **`test_4_reward_consistency_integration`**\n",
    "   - **Purpose:** End-to-end reward pipeline validation across multiple steps\n",
    "   - **Method:** Simulates 3-step episode with varying queue sizes\n",
    "   - **Inputs:** Sequential states: 0→10→7→11 vehicles\n",
    "   - **Critical invariant:** Sum of rewards equals net vehicle flow\n",
    "   - **Formula verified:** `r1 + r2 + r3 = (0 - 11) = -11`\n",
    "\n",
    "5. **`test_5_termination_flags`**\n",
    "   - **Purpose:** Verifies episode termination logic\n",
    "   - **Method:** Tests three scenarios with mocked completion flags\n",
    "   - **Inputs:**\n",
    "     - Normal: `completed=False, gui_closed=False`\n",
    "     - Natural end: `completed=True`\n",
    "     - GUI closure: `gui_closed=True`\n",
    "   - **Assertions:** Correct `terminated` and `truncated` flags\n",
    "\n",
    "6. **`test_6_reset_initialization`**\n",
    "   - **Purpose:** Ensures clean episode initialization\n",
    "   - **Method:** Corrupts state then calls reset\n",
    "   - **Inputs:** Simulation factory mock\n",
    "   - **Assertions:**\n",
    "     - New simulation created with correct parameters\n",
    "     - Cache reset to 0\n",
    "     - Valid initial state returned\n",
    "\n",
    "\n",
    "### Reproducing Tests\n",
    "\n",
    "**Run complete test suite:**\n",
    "```bash\n",
    "python test_environment.py\n",
    "```\n",
    "\n",
    "\n",
    "**Run specific test:**\n",
    "```bash\n",
    "python -m unittest test_environment.TestEnvironmentCritical.test_1_reward_calculation_correctness\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.7+\n",
    "- `unittest` (standard library)\n",
    "- `unittest.mock` (standard library)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
