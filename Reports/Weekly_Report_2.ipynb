{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e13e7b",
   "metadata": {},
   "source": [
    "# Weekly Report 2\n",
    "\n",
    "## What did I do this week?\n",
    "This week I implemented all the core reinforcement learning components. I wrote three main files:\n",
    "- `Q_learn.py`: Complete Q-learning agent with epsilon-greedy action selection and temporal difference learning\n",
    "- `environment.py`: RL environment wrapper that interfaces with the traffic simulator, constructs state observations, and calculates rewards\n",
    "- `utils.py`: Training and evaluation utilities including the training loop, model persistence (save/load), and evaluation metrics collection\n",
    "\n",
    "## How has the program progressed?\n",
    "The program has progressed significantly. I now have a complete, functional reinforcement learning system:\n",
    "- The Q-learning agent maintains a Q-table and can select actions using epsilon-greedy policy\n",
    "- The environment properly wraps the traffic simulator and provides the standard RL interface (reset, step)\n",
    "- Training loop runs for configurable number of episodes and saves the learned Q-table\n",
    "- Evaluation mode loads a trained model and reports metrics (average waiting time, collision frequency)\n",
    "\n",
    "\n",
    "## What did I learn this week/today?\n",
    "I learned several important concepts:\n",
    "- **Q-learning update rule**: How to properly implement the temporal difference update with learning rate, discount factor, and bootstrapping from the next state's maximum Q-value\n",
    "- **State caching**: The importance of maintaining `previous_queue_size` to calculate meaningful rewards across time steps\n",
    "- **Epsilon-greedy balance**: How to balance exploration (10%) vs exploitation (90%) for effective learning\n",
    "\n",
    "\n",
    "## What remains unclear or has been challenging?\n",
    "Some challenges I encountered:\n",
    "- **Convergence**: It's hard to know when the agent has converged - I'm using a fixed 10,000 episodes but don't have a principled stopping criterion\n",
    "- **State space explosion**: The Q-table grows large with many unique states. I'm not sure if I should implement state discretization/binning\n",
    "- **Evaluation variance**: Results vary significantly between evaluation runs due to stochastic traffic patterns\n",
    "- **Hyperparameter tuning**: I chose hyperparameters somewhat arbitrarily - I should do systematic grid search but it's computationally expensive\n",
    "\n",
    "## What will I do next?\n",
    "Next week I will:\n",
    "- Implement comprehensive unit tests for the environment class (reward calculation, state structure, termination conditions)\n",
    "- Write the project documentation (specification, implementation, testing documents)\n",
    "- Possibly experiment with different reward functions if time permits\n",
    "\n",
    "Chatgpt was used for rewriting in a clearly and correctly way  the concepts illustrated in this weekly report.\n",
    "\n",
    "---\n",
    "\n",
    "**Hours logged this week:** 18 hours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
